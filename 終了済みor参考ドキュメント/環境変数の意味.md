### 設定の読み込み規則（重要）
- **読み込み元**: `.env` が読み込まれます（サンプルは `env.example`）。未使用キーは無視されます。大文字/小文字は区別しません。  
```104:112:src/backend/config.py
    model_config = SettingsConfigDict(
        env_file=".env",
        extra="ignore",
        case_sensitive=False,
    )
```
- **フィールド名と環境変数名の対応**: `llm_provider` ⇔ `LLM_PROVIDER` のように、スネークケース⇔大文字アンダースコアで自動対応します。
- 以降の既定値は「1) env.example のサンプル値」「2) コード側のデフォルト値」の両方を明記します（実際は `.env` による上書きが有効）。

---

### 1) ENVIRONMENT
- 用途: 実行環境の識別（ロギング方針や運用切替のフラグに利用可能）。現時点では主にメタ情報。
- 既定値: env.example=development / コード既定="development"
- 使われ方: `src/backend/config.py` のみで保持（直接の分岐は現状なし）。
- 未設定/誤設定: 未設定なら "development"。誤設定でも致命的影響なし。
- 設定例: 本番で `"production"`。

---

### 2) LLM_PROVIDER, LLM_MODEL, LLM_TIMEOUT_MS, LLM_MAX_RETRIES
- 用途:
  - `LLM_PROVIDER`: LLM クライアントの選択。`openai` | `local`
  - `LLM_MODEL`: 使うモデル名
  - `LLM_TIMEOUT_MS`: 1回の LLM 呼び出しのタイムアウト
  - `LLM_MAX_RETRIES`: LLM 呼び出しの最大リトライ回数
- 既定値:
  - env.example: `LLM_PROVIDER=local`, `LLM_MODEL=gpt-5-mini`, `LLM_TIMEOUT_MS=8000`, `LLM_MAX_RETRIES=2`
  - コード既定: `llm_provider="openai"`, `llm_model="gpt-5-mini"`, `llm_timeout_ms=8000`, `llm_max_retries=2`
- 使われ方:
  - プロバイダ選択と初期化
```128:146:src/backend/providers.py
    provider = (settings.llm_provider or "").lower()
    try:
        if provider == "openai" and settings.openai_api_key:
            _LLM_INSTANCE = _llm_with_policy(_OpenAILLM(api_key=settings.openai_api_key, model=settings.llm_model))
            return _LLM_INSTANCE
        # 未対応プロバイダはローカルフォールバック
        _LLM_INSTANCE = _llm_with_policy(_LocalEchoLLM())
        return _LLM_INSTANCE
```
  - タイムアウト/リトライの適用（試行ごとにタイムアウト、指数的ではない短いバックオフ）
```99:111:src/backend/providers.py
        def complete(self, prompt: str) -> str:
            last_exc: Exception | None = None
            for attempt in range(1, max(1, settings.llm_max_retries) + 1):
                try:
                    future = _llm_executor.submit(llm.complete, prompt)
                    return future.result(timeout=settings.llm_timeout_ms / 1000.0)
                except Exception as exc:
                    last_exc = exc
                    if attempt >= max(1, settings.llm_max_retries):
                        break
                    time.sleep(0.1 * attempt)
            return ""
```
- 未設定/誤設定:
  - `LLM_PROVIDER=openai` でも `OPENAI_API_KEY` が空なら、自動で安全なローカルフォールバック（空文字を返す）に切替。
  - 失敗時は例外を投げず空文字を返す設計（上位はフォールバック前提）。
- 設定例:
  - OpenAI: `LLM_PROVIDER=openai`, `LLM_MODEL=gpt-4o-mini` など + `OPENAI_API_KEY`
  - オフライン学習/テスト: `LLM_PROVIDER=local`

---

### 3) EMBEDDING_PROVIDER, EMBEDDING_MODEL
- 用途: ベクトル検索用の埋め込み生成のプロバイダとモデル指定。
- 既定値: env.example=`openai`/`text-embedding-3-small`、コード既定も同じ。
- 使われ方:
```162:179:src/backend/providers.py
    provider = (settings.embedding_provider or "").lower()
    if provider == "openai" and settings.openai_api_key and OpenAI is not None:
        client = OpenAI(api_key=settings.openai_api_key)
        model = settings.embedding_model
        class _OpenAIEmbedding:
            def __call__(self, texts: List[str]) -> List[List[float]]:
                ...
                resp = client.embeddings.create(model=model, input=chunk)
                ...
        return _OpenAIEmbedding()
    return SimpleEmbeddingFunction()
```
- 未設定/誤設定: OpenAI キーが無い/SDK未導入なら、決定的なダミー埋め込み（次元8）に自動フォールバック。
- 設定例: OpenAI を使う場合 `EMBEDDING_PROVIDER=openai`, `EMBEDDING_MODEL=text-embedding-3-small` か `text-embedding-3-large` など。

---

### 4) OPENAI_API_KEY
- 用途: OpenAI（LLM/Embeddings）の認証。
- 既定値: 空（必須の場合はユーザーが設定）
- 使われ方: 上記 LLM/埋め込みの分岐で参照。
- 未設定/誤設定: 自動フォールバック（LLM は local、Embeddings はダミー）。

---

 

---

### 6) RAG_ENABLED / RAG_TIMEOUT_MS / RAG_MAX_RETRIES / RAG_RATE_LIMIT_PER_MIN
- 用途:
  - `RAG_ENABLED`: RAG パイプラインの有効化スイッチ
  - `RAG_TIMEOUT_MS`: 近傍検索（Chroma）の1回あたりタイムアウト
  - `RAG_MAX_RETRIES`: 近傍検索の最大リトライ
  - `RAG_RATE_LIMIT_PER_MIN`: RAG 呼び出しの毎分上限（トークンバケット）
- 既定値: env.example とコード既定は同値（true, 1500ms, 2, 120）
- 使われ方:
```395:401:src/backend/providers.py
    if not settings.rag_enabled or client is None:
        return None
    timeout_ms = timeout_ms if timeout_ms is not None else settings.rag_timeout_ms
    max_retries = max_retries if max_retries is not None else settings.rag_max_retries
    if not _rag_rate_limiter.allow():
        return None
```
```353:356:src/backend/providers.py
_rag_rate_limiter = TokenBucketRateLimiter(settings.rag_rate_limit_per_min)
_rag_executor = ThreadPoolExecutor(max_workers=4)
```
- 影響範囲: ルーター/フロー側で `rag_enabled` を見て接続可否を切替。
```18:21:src/backend/routers/text.py
    chroma_client = ChromaClientFactory().create_client() if settings.rag_enabled else None
    llm = get_llm_provider()
```
- 未設定/誤設定:
  - 無効化（false）で RAG は完全にスキップ。
  - タイムアウトやリトライを超えると `None` 返却→フロー側はフォールバックの引用を1件付与して成立。
- 設定例: 本番で外部 RAG を多用するなら `RAG_TIMEOUT_MS`/`RAG_MAX_RETRIES` を少し増やす、`RAG_RATE_LIMIT_PER_MIN` は課金や負荷に応じて調整。

---

### 7) CHROMA_PERSIST_DIR / CHROMA_SERVER_URL
- 用途:
  - `CHROMA_PERSIST_DIR`: ローカル永続ディレクトリ（サーバURL未指定時に使用）
  - `CHROMA_SERVER_URL`: リモートの Chroma サーバーを使う場合の接続先
- 既定値: env.example は `.chroma`、URL はコメントアウト（未指定）
- 使われ方:
```190:206:src/backend/providers.py
    def __init__(self, persist_directory: Optional[str] = None) -> None:
        self.persist_directory = persist_directory or settings.chroma_persist_dir
    def create_client(self) -> Any | None:
        ...
        if getattr(settings, "chroma_server_url", None):
            ...
            underlying = http_cls(host=settings.chroma_server_url)
        else:
            underlying = chromadb.PersistentClient(path=self.persist_directory)
```
- 未設定/誤設定:
  - SDK未導入/接続失敗時は軽量なインメモリ互換クライアントに自動フォールバックし、動作継続。
- 設定例:
  - ローカル永続: `CHROMA_SERVER_URL` 未設定、`CHROMA_PERSIST_DIR=.chroma`
  - リモート接続: `CHROMA_SERVER_URL=https://chroma.example`（この場合 persist は使わない）

---

### 8) SRS_DB_PATH / SRS_MAX_TODAY
- 用途:
  - `SRS_DB_PATH`: 復習（SRS）の SQLite DB の保存先
  - `SRS_MAX_TODAY`: 1日の出題上限（将来用のパラメータ）
- 既定値: env.example は `.data/srs.sqlite3` と `5`、コード既定も同じ
- 使われ方:
```400:402:src/backend/srs.py
# module-level singleton store (wired to settings)
store = SRSSQLiteStore(db_path=settings.srs_db_path)
```
  - DB パスは起動時にディレクトリ作成と DB 初期化。  
  - 現状、`SRS_MAX_TODAY` はコード内で参照されていません（未配線）。ルーターは固定値 5 を返します。
```21:26:src/backend/routers/review.py
async def review_today() -> ReviewTodayResponse:
    """Return today's review items (up to 5)."""
    items = store.get_today(limit=5)
```
- 未設定/誤設定:
  - `SRS_DB_PATH`: 書き込み不可な場所はエラー要因。既定の `.data/` 配下推奨。
  - `SRS_MAX_TODAY`: 現状は未使用なので影響なし（将来ワイヤリング予定）。
- 設定例: `SRS_DB_PATH=.data/srs.sqlite3`

---

### 9) RATE_LIMIT_PER_MIN_IP / RATE_LIMIT_PER_MIN_USER
- 用途: API 全体のレート制限（トークンバケット、毎分補充）。IP 単位/User 単位（`X-User-Id` ヘッダ）。
- 既定値: env.example=240/240、コード既定も同じ
- 使われ方（ミドルウェアに注入）:
```42:47:src/backend/main.py
app.add_middleware(
    RateLimitMiddleware,
    ip_capacity_per_minute=settings.rate_limit_per_min_ip,
    user_capacity_per_minute=settings.rate_limit_per_min_user,
)
```
- 429 時のレスポンスヘッダ（残量など）:
```118:126:src/backend/middleware.py
response.headers.setdefault("X-RateLimit-Limit-Ip", str(self._ip_capacity))
response.headers.setdefault("X-RateLimit-Remaining-Ip", str(remaining_ip))
...
```
- 未設定/誤設定:
  - 値は1以上にクランプ。誤って 0 等を設定しても最低 1 になります。
  - User 制限は `X-User-Id` が無いと適用されない（IP 制限のみ）。
- 設定例: 公開環境で `RATE_LIMIT_PER_MIN_IP=120`, `RATE_LIMIT_PER_MIN_USER=240` など。

---

### 10) SENTRY_DSN
- 用途: Sentry にログ/例外を送信（任意）。
- 既定値: 空（無効）
- 使われ方:
```27:36:src/backend/logging.py
if settings.sentry_dsn:
    import sentry_sdk
    from sentry_sdk.integrations.logging import LoggingIntegration
    sentry_logging = LoggingIntegration(level=logging.INFO, event_level=logging.ERROR)
    sentry_sdk.init(dsn=settings.sentry_dsn, integrations=[sentry_logging])
```
- 未設定/誤設定: 未設定なら無効、初期化失敗でもアプリは継続。
- 設定例: `SENTRY_DSN=https://<public_key>@<host>/<project>`

---

### よくある組み合わせ（実運用のヒント）
- 開発（オフライン想定）:
  - `LLM_PROVIDER=local`, `EMBEDDING_PROVIDER=openai`（キー無しでもダミー動作）
  - `RAG_ENABLED=true`（Chroma 未導入でもインメモリで動作）
- ステージング/本番（OpenAI）:
  - `LLM_PROVIDER=openai`, `LLM_MODEL=gpt-4o-mini`, `OPENAI_API_KEY=...`
  - `EMBEDDING_PROVIDER=openai`, `EMBEDDING_MODEL=text-embedding-3-small`
- 負荷制御:
  - API 全体は `RATE_LIMIT_PER_MIN_*`、RAG は `RAG_RATE_LIMIT_PER_MIN` で個別制御
- 監視:
  - 必要に応じて `SENTRY_DSN` を有効化

---

- 以上、`env.example` の各変数が「どこでどう使われ、未設定時にどう振る舞うか」まで落とし込んで説明しました。さらに掘り下げたい項目（例: Azure の具体的デプロイ手順、Chroma のサーバーモード運用など）があれば言ってください。

- 主要箇所の要点
  - `.env` を読み、未使用キーは無視・大文字小文字は不問。
  - LLM と Embeddings はキー未設定でも安全にフォールバック。
  - RAG は有効化フラグ＋タイムアウト/リトライ/レート制限で保護され、失敗時は静かにフォールバック。
  - API 全体のレート制限は IP と User の二系統。
  - `SRS_MAX_TODAY` は現状未使用（将来の出題上限パラメータ）。